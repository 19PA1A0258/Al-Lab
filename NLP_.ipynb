{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "kernelspec": {
      "display_name": "Python 3",
      "language": "python",
      "name": "python3"
    },
    "language_info": {
      "codemirror_mode": {
        "name": "ipython",
        "version": 3
      },
      "file_extension": ".py",
      "mimetype": "text/x-python",
      "name": "python",
      "nbconvert_exporter": "python",
      "pygments_lexer": "ipython3",
      "version": "3.8.3"
    },
    "colab": {
      "name": "NLP .ipynb",
      "provenance": [],
      "include_colab_link": true
    }
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/19PA1A0258/Al-Lab/blob/master/NLP_.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "8XTiiBA7pMYv"
      },
      "source": [
        "# 8) Natural Language processing\n",
        "Natural Language Processing is the technology used to aid computers to understand the humanâ€™s natural language like English. The objective of NLP is to read, understand, and make sense of the human languages. NLP techniques rely on machine learning to derive meaning from human languages.\n",
        "\n",
        "#### NLTK is a python package to handle text data.\n",
        "\n",
        "## a) Use NLTK package and perform the following\n",
        "* Tokenization\n",
        "* Stemming\n",
        "* Lemmatization\n",
        "* Bag of words\n",
        "* TF/IDF\n",
        "\n",
        "## b) Given set of documents, use NLTK to classify them.\n",
        "\n",
        "Sentence tokenization\n",
        "Word tokenization\n",
        "Frequency distribution of words in a document\n",
        "Stop words\n",
        "Stemming\n",
        "Normalization\n",
        "POS Tagger\n",
        "\n",
        "\n",
        "At the end Sentiment analysis is performed on movie reviews dataset using Naivebayes theorm.\n",
        "\n",
        "TF-IDF is used for word vectorization"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "IG1tOBmepMY2"
      },
      "source": [
        "#import nltk package\n",
        "import nltk"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "j80kF1M3pMY3"
      },
      "source": [
        "## 1)Tokenization\n",
        "Tokenization is the first step in text analytics. The process of breaking down a text paragraph into smaller chunks such as words or sentence is called Tokenization. Token is a single entity that is building blocks for sentence or paragraph."
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "ozvnGTWUpMY3"
      },
      "source": [
        "## 1 a) Sentence Tokenization\n",
        "Sentence tokenizer breaks text paragraph into sentences.\n",
        "\n",
        "Note: You should download **nltk.download('punkt')** before using \"sentence tokenizer.\n",
        "\n",
        "This tokenizer divides a text into a list of sentences, by using an unsupervised algorithm to build a model for abbreviation words, collocations, and words that start sentences. "
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "a2guH4BwpMY4",
        "outputId": "cb847988-e908-442f-a81b-fa2d456cc234"
      },
      "source": [
        "from nltk.tokenize import sent_tokenize\n",
        "# nltk.download('punkt')\n",
        "text=\" I study in Vishnu Institute of Technology. VITB rocks.\"\n",
        "tokenized_text=sent_tokenize(text)\n",
        "print(tokenized_text)"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "[' I study in Vishnu Institute of Technology.', 'VITB rocks.']\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "1zKV3QWipMY5"
      },
      "source": [
        "## 1 b) Word Tokenization\n",
        "Word tokenizer breaks text paragraph into words."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "i6kQ_8a9pMY5",
        "outputId": "9ece699a-92c1-467e-b14f-b9adbeb0dbd6"
      },
      "source": [
        "from nltk.tokenize import word_tokenize\n",
        "\n",
        "tokenized_word=word_tokenize(text)\n",
        "print(tokenized_word)"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "['I', 'study', 'in', 'Vishnu', 'Institute', 'of', 'Technology', '.', 'VITB', 'rocks', '.']\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "LAoSl3mApMY6"
      },
      "source": [
        "## 2) Stop words\n",
        "\n",
        "Stopwords considered as noise in the text. Text may contain stop words such as **is, am, are, this, a, an, the** etc.\n",
        "\n",
        "In NLTK for removing stopwords, you need to create a list of stopwords and filter out your list of tokens from these words."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "dbi-w8D9pMY6",
        "outputId": "7fbbed6d-a3ef-4ed0-ca7b-5f21d02e2b47"
      },
      "source": [
        "#let's list out stopwords available in NLTK package. \n",
        "\n",
        "from nltk.corpus import stopwords\n",
        "nltk.download('stopwords')\n",
        "stop_words=set(stopwords.words(\"english\"))\n",
        "print(stop_words)"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "{'myself', 'hasn', 'at', 'are', 'ain', \"couldn't\", 'their', 't', 'our', 'my', 'those', 'mustn', \"aren't\", 'for', 'it', 'this', 'too', 'was', 'being', 'ma', 'yourselves', 'am', 'while', 'now', 'isn', 'can', 'me', 'or', 'theirs', 'by', 'whom', 'm', 'then', 'over', 'ours', 'having', 'don', 'the', 'most', 'against', 'its', \"you've\", 'of', 'weren', 'with', 'an', \"haven't\", 'been', 'his', 'such', 'himself', \"weren't\", 'some', 'where', 'further', 'o', 'any', \"hadn't\", 'wouldn', 'd', 'do', 'more', 'which', 'have', 'should', 'he', 'between', \"should've\", 'about', 'him', 'who', 'is', 'again', 'these', 'after', 'both', 'she', 'them', 'into', 'doing', \"mightn't\", \"shouldn't\", 'and', \"shan't\", 'same', 'why', 'you', 'a', 'has', 'once', 'than', \"didn't\", 'yourself', 'that', 'because', 've', 'couldn', 'itself', 'didn', 'there', 'aren', 'how', 'they', 'won', 'doesn', \"wouldn't\", 'needn', 'before', 'did', 'above', 'herself', 'out', 's', \"you'd\", 'through', 'not', 'mightn', 'in', 'll', \"it's\", \"hasn't\", 'under', 'just', 'if', 'yours', 'on', 'will', 'off', 'from', \"that'll\", 'up', 'nor', 'own', 'few', 'all', 'haven', 'so', 'but', 'we', 'here', \"isn't\", 'shouldn', 'only', \"she's\", \"won't\", \"you'll\", 'as', \"mustn't\", 'ourselves', 'down', 'had', 'hadn', 'i', 'themselves', 'very', \"doesn't\", 'your', 'were', 'below', 'her', 'y', 'shan', \"you're\", 'what', 'hers', 'does', 'each', \"wasn't\", 'until', \"don't\", 'wasn', 'to', 'when', 'during', \"needn't\", 'no', 're', 'be', 'other'}\n"
          ],
          "name": "stdout"
        },
        {
          "output_type": "stream",
          "text": [
            "[nltk_data] Downloading package stopwords to\n",
            "[nltk_data]     C:\\Users\\immid\\AppData\\Roaming\\nltk_data...\n",
            "[nltk_data]   Package stopwords is already up-to-date!\n"
          ],
          "name": "stderr"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "qnmpPUM2pMY7",
        "outputId": "cd44071c-a54c-4ad6-f2b1-a942b338d13c"
      },
      "source": [
        "#let's list the tokenized sentence and the filtered one\n",
        "\n",
        "filtered_sent=[]\n",
        "for w in tokenized_word:\n",
        "  if w not in stop_words:\n",
        "    filtered_sent.append(w)\n",
        "print(\"Tokenized version:\",tokenized_word)\n",
        "print(\"\\n\")\n",
        "print(\"Filterd Version (After removing stopwords):\",filtered_sent)"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Tokenized version: ['I', 'study', 'in', 'Vishnu', 'Institute', 'of', 'Technology', '.', 'VITB', 'rocks', '.']\n",
            "\n",
            "\n",
            "Filterd Version (After removing stopwords): ['I', 'study', 'Vishnu', 'Institute', 'Technology', '.', 'VITB', 'rocks', '.']\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "frmxGupqpMY8"
      },
      "source": [
        "## 2) Normalization\n",
        "There are 2 types of normalizations\n",
        "\n",
        "* Stemming\n",
        "* Lemmatization"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "zjURhQKmpMY8"
      },
      "source": [
        "## Stemming\n",
        "Stemming is a process of linguistic normalization, which reduces words to their root word or chops off the derivational affixes. For example, **connection, connected, connecting** word reduce to a common word **\"connect\"**."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "e0JDSAUbpMY8",
        "outputId": "80c9ce62-a094-4e07-fb47-9ed89d20178a"
      },
      "source": [
        "from nltk.stem import PorterStemmer\n",
        "\n",
        "ps = PorterStemmer()\n",
        "stemmed_words=[]\n",
        "for w in filtered_sent:\n",
        "  stemmed_words.append(ps.stem(w))\n",
        "print(\"Filtered Sentence:\",filtered_sent)\n",
        "print(\"\\n\")\n",
        "print(\"Stemmed Sentence:\",stemmed_words)"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Filtered Sentence: ['I', 'study', 'Vishnu', 'Institute', 'Technology', '.', 'VITB', 'rocks', '.']\n",
            "\n",
            "\n",
            "Stemmed Sentence: ['I', 'studi', 'vishnu', 'institut', 'technolog', '.', 'vitb', 'rock', '.']\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "GsKVwvqOpMY9"
      },
      "source": [
        "## Lemmatization\n",
        "Lemmatization reduces words to their base word, which is linguistically correct lemmas. It transforms root word with the use of vocabulary and morphological analysis. Lemmatization is usually more sophisticated than stemming. Stemmer works on an individual word without knowledge of the context. For example, The word \"better\" has \"good\" as its lemma. This thing will miss by stemming because it requires a dictionary look-up."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "5X9Nf2ylpMY9",
        "outputId": "e563bd6e-9a01-4203-b53a-7af1bee7ef22"
      },
      "source": [
        "from nltk.stem.wordnet import WordNetLemmatizer\n",
        "# nltk.download('wordnet')\n",
        "\n",
        "lem = WordNetLemmatizer()\n",
        "lemmitized_words=[]\n",
        "for w in filtered_sent:\n",
        "  lemmitized_words.append(lem.lemmatize(w))\n",
        "print(\"Filtered Sentence:\",filtered_sent)\n",
        "print(\"\\n\")\n",
        "print(\"lemmitizer Sentence:\",lemmitized_words)"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Filtered Sentence: ['I', 'study', 'Vishnu', 'Institute', 'Technology', '.', 'VITB', 'rocks', '.']\n",
            "\n",
            "\n",
            "lemmitizer Sentence: ['I', 'study', 'Vishnu', 'Institute', 'Technology', '.', 'VITB', 'rock', '.']\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "-4fufWkjpMY9"
      },
      "source": [
        "## 3) Bag of words\n",
        "For the bag of words implementation, we use **CountVectorizer** from scikit-learn, which counts the frequency of each word present in our pre-processed dataset."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "hbu8fSxDpMY9"
      },
      "source": [
        "from sklearn.feature_extraction.text import CountVectorizer\n",
        "data = CountVectorizer(max_features=2000)\n",
        "X = data.fit_transform(lemmitized_words).toarray()"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "h4Iw5oiWpMY-",
        "outputId": "5332ffd8-2ff6-4530-f36f-3b0d9bd3ea28"
      },
      "source": [
        "X"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "array([[0, 0, 0, 0, 0, 0],\n",
              "       [0, 0, 1, 0, 0, 0],\n",
              "       [0, 0, 0, 0, 1, 0],\n",
              "       [1, 0, 0, 0, 0, 0],\n",
              "       [0, 0, 0, 1, 0, 0],\n",
              "       [0, 0, 0, 0, 0, 0],\n",
              "       [0, 0, 0, 0, 0, 1],\n",
              "       [0, 1, 0, 0, 0, 0],\n",
              "       [0, 0, 0, 0, 0, 0]], dtype=int64)"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 42
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "sCat66CWpMY-",
        "outputId": "e66df3e7-136e-4575-de2b-c1a5c47afb3f"
      },
      "source": [
        "data.vocabulary_"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "{'study': 2,\n",
              " 'vishnu': 4,\n",
              " 'institute': 0,\n",
              " 'technology': 3,\n",
              " 'vitb': 5,\n",
              " 'rock': 1}"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 43
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "sRayDxIdpMY-"
      },
      "source": [
        "\n",
        "\n",
        "![1_GIb1j98BvG5oLjMqWMC3NA.png](attachment:1_GIb1j98BvG5oLjMqWMC3NA.png)"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "JlEX7859pMY-"
      },
      "source": [
        "# Import pandas\n",
        "import pandas as pd"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "35TavlnHpMY_",
        "outputId": "cd7bfcfa-f70e-4b62-ad50-615a795f14b1"
      },
      "source": [
        "data=pd.read_csv('train.tsv', sep='\\t')\n",
        "data.head()"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/html": [
              "<div>\n",
              "<style scoped>\n",
              "    .dataframe tbody tr th:only-of-type {\n",
              "        vertical-align: middle;\n",
              "    }\n",
              "\n",
              "    .dataframe tbody tr th {\n",
              "        vertical-align: top;\n",
              "    }\n",
              "\n",
              "    .dataframe thead th {\n",
              "        text-align: right;\n",
              "    }\n",
              "</style>\n",
              "<table border=\"1\" class=\"dataframe\">\n",
              "  <thead>\n",
              "    <tr style=\"text-align: right;\">\n",
              "      <th></th>\n",
              "      <th>PhraseId</th>\n",
              "      <th>SentenceId</th>\n",
              "      <th>Phrase</th>\n",
              "      <th>Sentiment</th>\n",
              "    </tr>\n",
              "  </thead>\n",
              "  <tbody>\n",
              "    <tr>\n",
              "      <th>0</th>\n",
              "      <td>1</td>\n",
              "      <td>1</td>\n",
              "      <td>A series of escapades demonstrating the adage ...</td>\n",
              "      <td>1</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>1</th>\n",
              "      <td>2</td>\n",
              "      <td>1</td>\n",
              "      <td>A series of escapades demonstrating the adage ...</td>\n",
              "      <td>2</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>2</th>\n",
              "      <td>3</td>\n",
              "      <td>1</td>\n",
              "      <td>A series</td>\n",
              "      <td>2</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>3</th>\n",
              "      <td>4</td>\n",
              "      <td>1</td>\n",
              "      <td>A</td>\n",
              "      <td>2</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>4</th>\n",
              "      <td>5</td>\n",
              "      <td>1</td>\n",
              "      <td>series</td>\n",
              "      <td>2</td>\n",
              "    </tr>\n",
              "  </tbody>\n",
              "</table>\n",
              "</div>"
            ],
            "text/plain": [
              "   PhraseId  SentenceId                                             Phrase  \\\n",
              "0         1           1  A series of escapades demonstrating the adage ...   \n",
              "1         2           1  A series of escapades demonstrating the adage ...   \n",
              "2         3           1                                           A series   \n",
              "3         4           1                                                  A   \n",
              "4         5           1                                             series   \n",
              "\n",
              "   Sentiment  \n",
              "0          1  \n",
              "1          2  \n",
              "2          2  \n",
              "3          2  \n",
              "4          2  "
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 16
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "64UNMlAHpMY_",
        "outputId": "9fee22a8-73ce-4209-b009-6e1325c06644"
      },
      "source": [
        "data.Sentiment.value_counts()"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "2    79582\n",
              "3    32927\n",
              "1    27273\n",
              "4     9206\n",
              "0     7072\n",
              "Name: Sentiment, dtype: int64"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 17
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "cq5jniJlpMY_"
      },
      "source": [
        "from sklearn.feature_extraction.text import TfidfVectorizer\n",
        "tf=TfidfVectorizer()\n",
        "text_tf= tf.fit_transform(data['Phrase'])"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "y2i7WsGhpMY_"
      },
      "source": [
        "from sklearn.model_selection import train_test_split\n",
        "X_train, X_test, y_train, y_test = train_test_split(text_tf, data['Sentiment'], test_size=0.3, random_state=123)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "8_kE4yBOpMZA",
        "outputId": "84250578-728a-4540-89c4-feaf078d0929"
      },
      "source": [
        "from sklearn.naive_bayes import MultinomialNB\n",
        "from sklearn import metrics\n",
        "# Model Generation Using Multinomial Naive Bayes\n",
        "clf = MultinomialNB().fit(X_train, y_train)\n",
        "predicted= clf.predict(X_test)\n",
        "print(\"MultinomialNB Accuracy:\",metrics.accuracy_score(y_test, predicted))"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "MultinomialNB Accuracy: 0.5865265496176684\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "4uUnhb2qpMZA"
      },
      "source": [
        ""
      ],
      "execution_count": null,
      "outputs": []
    }
  ]
}